---
output:
  word_document: default
  html_document: default
  pdf_document: default
---
 
# Coursework MA321-7-SP: Initial Task - R code to get started (Version: 30th January 2024)
#
# Use setwd("") to change the working directory to the one you use on your computer
#
# If you use a computer in a University lab, make sure that you save all what you have
# done in a your network directory or on your computer (make sure that you have backups
# using for example the Apple Cloud - or other providers - or an additonal USB stick or other 
# means to back up your work. You are responsible that you have back ups of work.)
#
# Copy the file "gene-expression-invasive-vs-noninvasive-cancer.csv" from Moodle into your working directory

# Read the data into RStudio (or R) using the read.csv R command
```{r}

```



```{r}
InitialData <- read.csv(file="gene-expression-invasive-vs-noninvasive-cancer.csv")
```

# Check using the str, dim and dimnames command it worked - parts of the generated output 
# are added as comments; lines starting with "#" comments and not R code. 
```{r}
str(InitialData)
```
#
# 'data.frame':	78 obs. of  4949 variables:
# $ J00129        : num  -0.448 -0.48 -0.568 -0.819 -0.112 -0.391 -0.624 -0.528 -0.811 -0.839 ...
# $ Contig29982_RC: num  -0.296 -0.512 -0.411 -0.267 -0.67 -0.31 -0.12 -0.447 -0.536 2 ...
# ...
# 

```{r}
dim(InitialData)
```

# [1]   78 4949
# 

# Command -----------------------------------------------------------------

```{r}
dimnames(InitialData)[[2]][4947:4949]
```


# 
# [1] "NM_000898" "AF067420"  "Class" 
# 
# 
# The data set has 
#    78 rows (patients)
#  4949 columns with 4948 gene expression measurement of cancer tissue, each column representing a 'gene'
#               with column 4949 having the information of a class variable with two values: 1 and 2.
```{r}
table(InitialData[4949])
```


# 
# Class
# 1  2 
# 34 44 
# 


# To gurantee that your data narrative is individual we ask you to set the seed of the random 
# number generator with R function set.seed using your Registration Number; 
# for example "2244222" - replace by your registration number
```{r}
set.seed(2310158)
```
# 
# Note: R function set.seed generates no R output. Make sure that you run your final 
#      data analysis with the submitted R code and that all tables and figures are
#      are generated by the final code. You may use R Mark Down or similar R tools to support this.
# 
#  

# 
# Selects your random individual subset of 50 genes
# 

###1. (15 marks).
###1.(a) Compute the variance, co-variance and correlation matrix of your random subset of 10 genes. Add an appropriate table to your report.

```{r}
my.gene.subset <- InitialData[,rank(runif(1:4948))[1:10]]
str(my.gene.subset)
# To calculate variance of random subset of 10 genes.
variance<-var(my.gene.subset)
# To calculate co-variance of random subset of 10 genes.
covar.cov<-cov(as.matrix(my.gene.subset))
# To calculate correlation of random subset of 10 genes
correlation<-cor(as.matrix(my.gene.subset))

```
```{r}
library(knitr)

```


```{r}
# Creating a table to show the variance, covariance,correlation of the random 10 genes
Variance_table<-as.data.frame(variance)
kable(Variance_table,caption = "Summary of variance of 10 random genes")
covariance_table<-as.data.frame(covar.cov)
kable(covariance_table,caption = "Summary of covariance of 10 random genes")

correlation_table<-as.data.frame(correlation)
kable(correlation_table,caption = "Summary of correlation of 10 random genes")

```
The Variance matrix shows the variance between each gene helping to further research the biological combination between the different gene expressions. The variance value nearing zero shows significantly less variability.

The Covariance summary table shows the co-variability between two genes. Positive values refer that the pair genes are directly proportional. Negative values refer that the pair genes are inversely proportional. Zero values refer to relationships that tend to be zero or no relationship.

The Correlation summary table shows the strength of the linear relationship between two genes. The positive values nearing 1 indicate the positive correlation between the two genes. The negative values near -1 indicate a negative correlation. The values nearing to zero have minimal correlation or no relationship between the genes. The table shows the mixed expression level of correlation between various pair of gene.



###1.(b) Using R to calculate the distance matrix of your random subset of 10 genes. Add an appropriate table to your report.
```{r}
# To obtain the distance matrix for randomly selected 10 genes.
distance_matrix<-dist(scale(my.gene.subset))
print(distance_matrix)
```
A distance matrix is the distance matrix calculating the distance between each gene pair based on similarity and dissimilarity between the genes. This matrix refers to a quantitative measure of dissimilarity between pairs of genes. Thus, a higher value indicates greater dissimilarity. This matrix depicts the fundamental step to analyze the relationship between genes and gene expression data.



###(c) Using R to calculate univariate Q-Q-plots and a Q-Q-plot based on the generalised distance for the observations of your random subset of 10 genes. Add appropriate figures to your report.
```{r}
library(ggplot2)
library(reshape2)
```


```{r}
#Reshape the data set
data.reshape<-melt(my.gene.subset[1:10])

#Univariate Q-Q plot
QQplot.Univariate<-ggplot(data.reshape,aes(sample=value))+
  geom_qq(col='blue')+
  geom_qq_line(col='red')+
  facet_wrap(~ variable, scales = "free")+
  theme_minimal()+
  labs(title = 'Q-Q plots for each random 10 genes')
plot(QQplot.Univariate)
```
The univariate Q-Q plots are generated for a random subset of 10 genes. These plots depict the quantities of the observed data to the theoretical normal distribution. Each plot represent each gene of a random subset of 10 genes under the variable name my.gene.subset.

The blue points represent the observed quantities and the red line represents quantities of the normal distribution. When the blue points fall closely to the red line, inferring that the data nearly follows the normal distribution.While for some genes the blue points are deviated from the red line representing a high-tailed distribution.

This Q-Q plot graph helps to infer the normality assumptions and further statistical methods to apply to further research


```{r}
#To create Q-Q plots based on generalised distance
pairs(my.gene.subset)
# cm - mean value for column variable of the dataset.
cm <-colMeans(my.gene.subset)

# covariance matrix of the dataset 
S<-cov(my.gene.subset)

# Mahalanobis distances is used to calculate the distance between
#the point and the distribution
d<-apply(my.gene.subset,MARGIN = 1, function(my.gene.subset) +t(my.gene.subset-cm)%*% solve(S)%*%(my.gene.subset-cm))

# plot to assess Mahalanobis distance
plot(qchisq(((1:nrow(my.gene.subset))-1/2)/nrow(my.gene.subset), df=9),
     sort(d),
     xlab = expression(paste(chi[3]^2, "Quantile")),
     ylab = "Ordered distances");abline(a = 0, b = 1,col='dark Green',title("Q-Q plots on the generalised distance for 10 random genes"))
```
The pair plot graph shows the significant pairwise relationships between the randomly selected 10 genes. This gives us a clear picture to identify the patterns and outliers in the existing data structure.

The Mahalanobis distance is used to calculate the generalized distance here. The corresponding observations, mean values and covariance of the dataset are used. The Q-Q plot helps to assess the distribution using this Mahalanobis distance.

This plot compares the observed Mahalanobis distance to the chi-square distribution, which represents the degree of freedom as 9(10 random gene variable -1). The green line represents the reference line showing the exact match point of observed and expected quantiles. The observed Mahalanobis distance, mostly closely on the green line (reference line), depicted that they are chi-square distributed, further indicating a multivariate normality of the data set containing 10 random genes.

The few observations falling apart from the green line indicate that there are few outliers in the dataset. However, this doesn't affect the data much. This helps to evaluate the multivariate assumptions of observations in the dataset and any potential data anomalies before proceeding observations with further analysis.



###2. (15 marks). Use R for a principal component analysis of your random subset of 10 genes. Add appropriate tables and figures to your report.

```{r}
# To obtain Principal Component Analysis for the subset my.gene.subset
pca.mygenesubset<-princomp(my.gene.subset,cor = TRUE)

# Summarize the Principal Component Analysis with loadings and cutoff 
summary(pca.mygenesubset,loadings= TRUE,cutoff=.0)

# Graphical representation of the Principal Component Analysis
plot(pca.mygenesubset)
```
Principal Component Analysis is generally used to reduce the complex dimensions and identify the high dimensions of the dataset. The PCA is performed on the randomly selected 10 genes consisting of two main tasks in the analysis; importance of components and loading of each gene on the principal components.

The importance of the Components table shows the quantitative measure of standard deviation, Proportion of variance and the cumulative proportion of each component.

The standard deviation is calculated by the square root of eigenvalues of the covariance matrix. 

The proportion of variance is the square of the standard deviation divided by the sum of squares of all standard deviations.

The cumulative proportion of variance is the cumulative sum of proportions of variance explained by each principal component.

Loadings refers to the contribution of each gene to principal component. High value depicts the high association between the gene and the respective principal component.

In principal component 1, there exists the highest value in standard deviation, Proportion of Variance and Cumulative Proportion showing the high variability in the data. And the trend is decreasing in further component proceedings, which clearly indicates less variability in the data. Each component gives valuable information for analyzing the overall structure of this data set.

While analyzing the loadings in component 1, genes Contig11075_RC, NM_004358, and L36069 contribute significant variations in Component 1. Similarly, in Component 2, Contig54232_RC, NM_004056, Contig11075_RC and NM_001218 genes have higher values. The loading table illustrates the relationship between the genes and the principal components. The high loading values of genes represents high correlation and negative values are negatively correlated.

This PCA model gives us the underlying structure of this gene expression data. The repeated pattern of loading of genes to the components and analyzing the importance of components give us the key pattern and relationship between the genes. The graph clearly shows the variance in the data that has decreased with increasing components.


###3. (15 marks).


###3.(a) Fit a multivariate analysis of variance model (MANOVA) to your random subset of 10 genes. Investigate if there is a difference between invasive (label 1) and noninvasive (label 2) cancer. Note: You need to add column 4949 containing the information invasive and noninvasive cancer to your random subset of 10 genes. Add appropriate tables and figures to your report.


```{r}
#3.(a) To find the MANOVA of random subset of 10 genes.
#Null Hypothesis H0:There is no significant difference in the expression levels of the random subset of 10 genes between invasive (label 1) and noninvasive (label 2) cancer.
#Alternative Hypothesis (H1): There is a significant difference in the expression levels of the random subset of 10 genes between invasive (label 1) and noninvasive (label 2) cancer.
my.gene.subset$class<-InitialData$Class
my.gene.subset$class
x<-as.matrix(my.gene.subset[,c(1:10)])

# To fit the MANOVA
Manova.fit<-manova(x~my.gene.subset$class)

# Summarize the MANOVA fit
summary(Manova.fit,intercept=TRUE)
```
The MANOVA fit model helps to investigate whether there is a significant difference in the expression levels of 10 random genes between invasive and non-invasive cancer.

In MANOVA, intercepts represent the estimated intercepts of the regression equation for each dependent variable. Similar to the intercepts in linear regression models, the position where the expected variable of each dependent variable when all other independent variables tend to zero, intercepts are formed.

F-Statistics are determined to test the overall significance of the model. In the overall MANOVA test, F-statistics are calculated by comparing the variability between groups to the variability within groups.

The degree of freedom is associated with the number of groups, the number of dependent variables and parameters estimated in the MANOVA model. A multivariate test statistic termed Pillai trace is used in MANOVA to evaluate the overall significance of the model or individual effects. Ranges from 0 to 1 with larger values depicting greater discrimination between the groups. The p-value indicates the probability of observing the data with test statistics. Here, the smaller the p-value depicts that there are significant differences between the groups or effect in the respective model. Pr's value, otherwise known as the Probability Ratio, is another measure of significance. The ratio of the observed value of a test statistic to its expected value in the null hypothesis. If a Pr value close to 1 indicates a prediction of a null hypothesis is approved when the Pr values differ far from 1, the most likely null hypothesis is rejected.

The intercept of Pillai traces 0.54587, indicates a moderate-to-large effect size. The approximate F-statistic is 8.0534 and the degree of freedom is 10 and 67 for the numerator and denominator, respectively. The associated p-value (Pr(>F)) is highly significant (p < 0.001), showing that there is a significant overall effect when considering all variables.

The intercept of Pillai traces 0.17394 for variable class, which indicates smaller effect size when compared to the intercept.

The approximate F-statistic is 1.4108 and the degree of freedom is 10 and 67 for the numerator and denominator, respectively. The associated p-value (Pr(>F)) is 0.1948, which is not significant (p > 0.05). This suggests that there is no significant difference in gene expression levels of 10 random genes between invasive and noninvasive cancer after controlling for other variables.

The residual 76 shows the unexplained variances after counting on the intercepts.

To conclude, the MANOVA results show there is a significant impact while considering all variables, while specific variable-class (invasive vs. noninvasive cancer) does not significantly impact the differences in gene expression levels.

Therefore, the H0 null Hypothesis is not rejected, concluding that there is no significant difference in the expression levels of the random subset of 10 genes between invasive (label 1) and noninvasive (label 2) cancer.



```{r}
#Box plot representation of MANOVA

boxplot(x~my.gene.subset$class)
```
The above box plot shows the interquartile range of gene expression for each cancer type (invasive and noninvasive).

The Q1 and Q3 are the top and bottom edges of the box and the middle line(line inside the box) represents the median value of the gene expression. The data points scattered outside the box are termed as outliers.
Label 1 represents noninvasive cancer and Label 2 represents invasive cancer.

From the MANOVA table:

For the intercept (overall gene expression levels), the Pillai's trace statistic is 0.54587, indicating a significant difference between the two groups (p < 0.001).

For the factor my.gene.subset$class (cancer type), the Pillai's trace statistic is 0.17394, indicating a non-significant difference between the two groups (p = 0.1948).

The box plot shows substantial overlap in label 1 and label 2 which clearly shows that there is o significant difference in the expression levels of the random subset of 10 genes between invasive (label 1) and noninvasive (label 2) cancer. Hence, H0 Null hypothesis is accepted.


###3.(b) Use the first and second principal component to illustrate, if there is a difference between invasive and noninvasive cancer. Add appropriate tables and figures to your report.

```{r}

#3.b Taking the first two Principal components
pca.mygenesubset<-princomp(my.gene.subset,cor=TRUE)

# Extracting the coefficients for first and second principal components
coeff.PC1.PC2<-pca.mygenesubset$loadings[,1:2]

#Print the coefficients of first(PC1) and second(PC2) principal components
print(coeff.PC1.PC2)

```
```{r}
# Graphical representation of the difference between invasive vs noninvasive
plot(coeff.PC1.PC2)
```
```{r}
#
gene_class_var<-InitialData$Class

gene_exclu_class<-my.gene.subset[,1:10]
gene_pca<-prcomp(gene_exclu_class,scale=TRUE)

#Set a dataframe of extracted PC1 and PC2 from PCA results
gene_set<-as.data.frame(gene_pca$x[,1:2])
gene_set$class<-gene_class_var
# To convert class to factor variable for plotting purpose
gene_set$class<-as.factor(gene_set$class)
# ggplot to show the PCA plot
library(ggplot2)
ggplot(gene_set,aes(x=PC1,y=PC2,color=class))+geom_point(alpha=0.7)+
  theme_minimal()+
  labs(title="Principal Component Analysis of Gene Expression",
       x="Principal Component 1",
       y="Principal Component 2",
       color="Cancer Type")+
  scale_color_manual(values=c("1"="red","2"="blue"))
  
```
In order to compare the first and second principal components, we are extracting the coefficients of the first principal component (PC1) and second principal component (PC2). These coefficients are the contributions of each variable to their respective principal component. The positive coefficient represents positive correlation and the negative coefficient represents negative correlation.

The plot graph of coefficients of PC1 and PC2 helps us to visualize the difference between the two groups of invasive and noninvasive cancer types. PCA is performed on the extracted PC1 and PC2, which is a subset of genes excluding the class variables, and of ggplots are used to give a clear visualization of differences between the two groups, invasive and noninvasive cancer types.

The inferences from the above analysis is that the PCA coefficient showing the correlation between each gene and PC1 and PC2 and insights in the gene contribute more variation in PC1 and PC2. The graph of the coefficients of the first two principal components gives the relative contribution of each gene to overall variation in the dataset, which is more of a scattered plot without any overlaps.

However, through the PC1 and PC2 of gene expression in a reduced dimensional-space, we see there are significant clustered plot patterns providing insights into the potential discriminatory power of the selected subset (differentiated by red and blue points in the graph) of genes for distinguishing between invasive and noninvasive cancer types.


###4. (30 marks).

###4.(a) Apply LDA to your random subset of 10 genes and the class variable (invasive (label 1) and noninvasive (label 2) cancer). Calculate a confusion matrix, sensitivity, specificity and misclassification error. Add appropriate tables and figures to your report.

```{r}
#4.(a)
library(MASS)
library(caret)
```

```{r}
# Converting the class to factor type
my.gene.subset$class<-as.factor(my.gene.subset$class)
```

```{r}
# Split the data into training and testing sets (optional, but recommended for performance evaluation)
set.seed(2310158) # Use a seed for reproducibility
indexes <- createDataPartition(my.gene.subset$class, p = 0.7, list = FALSE)
Data.1 <- my.gene.subset[indexes, ]
Data.2 <- my.gene.subset[-indexes, ]
```

```{r}
# Training set = Data.1, Testing set= Data.2 ,data split in 70:30 ratio
# Apply LDA on the training data
lda.model<-lda(class ~.,data = Data.1)

# Predictions
Predict.lda<-predict(lda.model,Data.2)
Predict.class<-Predict.lda$class

#confusion matrix

Matrix.confusion<-confusionMatrix(Predict.class,Data.2$class)
print(Matrix.confusion)

```
```{r}
# Sensitivity, Specificity, and Misclassification Error from the confusion matrix
print(sensitivity.lda<-Matrix.confusion$byClass['sensitivity'])
print(specificity.lda<-Matrix.confusion$byClass['specificity'])
print(misclassify.lda.error<-1- Matrix.confusion$overall['Accuracy'])

```
The above output of the Linear Discriminant Analysis (LDA) helps to analyze the performance to classify the cancer types as invasive and noninvasive. LDA is applied to a subset of 10 random genes with class variables indicating invasive and noninvasive cancer types.

The data has been split into training represented as Data.1 (70 percentage of the data) and testing as Data.2(remaining 30 percentage of the data). LDA is applied to Data.1 training dataset with class variables as a response and predictors as gene expression features. The trained LDA model is used to predict the class labels of the testing set (data.2) of the data. In order to analyze the performance of the LDA model, a confusion matrix is computed.

The confusion matrix shows the numeric data of the performance of the LDA model as follows:

Accuracy is 34.78 percent, which is considerably low, referring to a low performance model. The 95 percent confidence interval for accuracy is between 16.38 percent to 57.27 percent, representing a poor model's predictive capabilities. The kappa statistic is -0.3424, indicating poor agreement beyond the chance of current and actual class. This show's model's performance is not significant. The sensitivity (true positive rate) is 20 percent, which is a measure of the proportion of actual invasive cases identified by the model. The specificity is 46.15 percent, which is a measure of the proportion of actual noninvasive identified by the model. The misclassification error is miscalculated instances which computes to 65.21 percent.

To conclude, the above LDA model trained has poor performance with the testing data. Further classification accuracy has to be improved by exploring more alternative classification algorithms.
 
###4.(b) Apply Quadratic discriminant analysis (QDA) to your random subset of 10 genes and the class variable (invasive (label 1) and noninvasive (label 2) cancer). Calculate a confusion matrix, sensitivity, specificity and misclassification error. Add appropriate tables and figures to your report.

```{r}
#4.(b)
#Set a subset from the gene expression data
x<-as.matrix(my.gene.subset[,c(1:10)])

#Fitting a QDA(Quadratic Discriminant Analysis) model 
#Predictor variable as specified gene expression, response variable as class
Qda.model<-qda(class~NM_015957+Contig23913_RC+Contig49076_RC+NM_003034+NM_012396+U81599+NM_004417+AL157424+NM_005512+Contig2339_RC , data = my.gene.subset)

#Print the QDA model details
print(Qda.model)


```
```{r}
# Predictions on QDA model
Predict.qdaclass<-predict(Qda.model)$class

# Creating a contingency table comparing predicted class to actual class
table(Predict.qdaclass,my.gene.subset$class)
```

```{r}
#QDA confusion matrix
Confusion.matrix.QDA<-confusionMatrix(Predict.qdaclass,my.gene.subset$class)
print(Confusion.matrix.QDA)
```
```{r}
#Sensitivity,specificity and misclassification error
Sensitivity.QDA<-Confusion.matrix.QDA$byClass['Sensitivity']
Specificity.QDA<-Confusion.matrix.QDA$byClass['Specificity']
misclassify.QDA.error<- 1- Confusion.matrix.QDA$overall['Accuracy']
print(Sensitivity.QDA)
print(Specificity.QDA)
print(misclassify.QDA.error)
```
The above Quadratic Discriminant Analysis (QDA) helps to develop a predictive model to classify the invasive and noninvasive cancer type based on the gene expression data. A subset of gene expression data is created and converted to the matrix 'x'. The prior probabilities of the group and group means are calculated. The QDA fitting is performed with the predictor variables specified as gene expressions, and the response variable as 'class'. The classes are predicted from the QDA fitted model.

A contingency table is created to compare the predicted classes with actual classes. A confusion matrix is generated to evaluate the performance of the QDA predictions. The classification is as follows:

 24 invasive cancer cases as True Positives,
 40 non-invasive cancer cases as True Negatives,
 4 non-invasive cancer cases as False Positives,
 10 invasive cancer cases as False Negatives.
 
The QDA model accuracy computes to 82.05 percent, which is significantly a high performance of the model. The 95 percent confidence interval for accuracy is between 71.72 percent to 89.73 percent, representing a decent model's predictive capabilities. The kappa statistic is 0.6276, which shows agreement between predicted and actual classes beyond chance turns to be moderate. Sensitivity for this QDA model is 70.59 percent, which determines the true positives of invasive cancer cases. Specificity identified by this QDA model is 90.91 percent of noninvasive cancer cases, which determines the ability to avoid false alarm in noninvasive cases.
The calculated misclassification error of the overall QDA model is 17.95 percent, which is the proportion of incorrectly classified cases.

In order to infer from this QDA model, there is a promising accurate classification of cancer types based on gene expression data. High performance, sensitivity and specificity values are remarkably high, depicting good performance in detecting invasive and noninvasive cancer cases.




###4.(c) Discuss the difference between LDA and QDA using the results on your random subset of 10 genes and the class variable (invasive (label 1) and noninvasive (label 2) cancer).

The LDA model is based on the assumption that the predictors have a multivariate normal distribution within each class and the covariance matrix across classes is equal. The QDA model proceeds with no assumption, allowing each class to have its own covariance matrix.

With respect to the accuracy of the overall model, the LDA model shows a low accuracy level of 34.78 percent comparatively on the other hand, the QDA model has a high accuracy of 82.05 percent. The kappa statistics for the LDA model is -0.3424 and the QDA model is 0.6276. The kappa statistic generally determines the agreement between the predicted and actual classes. In this case, the QDA model exhibits high agreement beyond the chance of the positive value 0.6276 to that of LDA model in negative value -0.3424.

The 95 percent confidence interval for accuracy range in the LDA model is low, between 16.38 percent to 57.27 percent, whereas the QDA model comes with a better accuracy range from 71.72 percent to 89.83 percent. It is evident that the LDA model has poor predictive capabilities when compared to the QDA model.

The Sensitivity(True Positive Rate), specificity(True Negative Rate) and misclassification error of the LDA model are 20 percent,46.15 percent and 65.21 percent. And the QDA model has 70.59 percent,90.91 percent and 17.94 percent respectively. Sensitivity and specificity are significantly higher in the QDA model compared to the LDA model. This is a true positive rate of invasive cancer cases and a true negative rate of noninvasive cancer cases in a random 10 gene subset. The misclassification error is comparatively lower in the QDA model than the LDA model, which determines that the QDA model possess more accurate classification of cancer types.
The LDA model approaches with the assumption that classes are linear. However, the QDA model showcases more flexibility for complex decisions, which leads to better classification performance.

In order to conclude, the above LDA and QDA models. In this case, the difference in assumption of the predictors component makes the QDA model to be more flexible with respect to the covariance matrix. This further enhances the performance of the QDA model compared to the LDA model for this analysis. The LDA model seems to be more promising for simpler computation of a dataset where the assumptions of equal covariance matrices is satisfied. But the QDA model has become a more optimal model to use due to its flexibility in negotiating the assumption of covariance matrices.

This supports the QDA model to be more reliable for complex relationship modelling between predictors and classes.




###5. (25 marks). Use the median of the first principal component of your random subset of 10 genes to predict the class variable (invasive (label 1) and noninvasive (label 2) cancer). Use Fisher’s Exact test and sensitivity,specificity and Youden index. Add appropriate tables and figures to your report.

```{r}
#5
# Performing PCA excluding the Class variable
Data.5<-my.gene.subset[,-ncol(my.gene.subset)]
Result<-prcomp(Data.5,scale=TRUE)
First.Component<-Result$x[,1]
my.gene.subset$class<-as.factor(my.gene.subset$class)


# Determining the median of PC1
Median.First.Component<-median(First.Component)

# Classifying median based on first component
Predictclassify.Median<-ifelse(First.Component>Median.First.Component,'1','0')#1 above median, 0 below median

Actualclassify.Median<-as.character(my.gene.subset$class)

#Null Hypothesis(H0):There is no association between the predicted class based on the first principal component and the actual class from the dataset.


#Alternative Hypothesis(H1):There is an association between the predicted class based on the first principal component and the actual class from the dataset.

# Fisher’s Exact Test
Fisher.table<-table(Predicted=Predictclassify.Median,Actual=Actualclassify.Median)
Fishertest.result<-fisher.test(Fisher.table)


# Convert to factors with consistent levels
Predictclassify.Median <- factor(Predictclassify.Median, levels = c("0", "1"))
Actualclassify.Median <- factor(Actualclassify.Median, levels = c("0", "1"))

# To determine Confusion matrix,sensitivity,specificity and youden_index
con_matrix<-confusionMatrix(Predictclassify.Median,Actualclassify.Median)
sensitivity <- con_matrix$byclass['Sensitivity']
specificity <- con_matrix$byclass['Specificity']
youden_index <- sensitivity + specificity - 1

print(con_matrix)
print(sensitivity)
print(specificity)
print(youden_index)

print(Fishertest.result)
```
The main aim of this analysis is to predict the relationship between predicted classifications based on the first principal component of gene expression data and the actual class using Fischer's exact test. Fischer's exact test is used to determine the significant association between two categorical values when the sample size is small and chi-square assumptions (no random sampling, low count in contingency table) are violated. A significant tool for analysis when other methods are not appropriate.

Null Hypothesis(H0): There is no association between the predicted class based on the first principal component and the actual class from the dataset.

Alternative Hypothesis(H1): There is an association between the predicted class based on the first principal component and the actual class from the dataset.

A new data frame is created as Data.5 by excluding the last column from my.gene.subset data frame, an essential step to perform Principal Component Analysis on the gene expression data only. Principal Component Analysis is performed on new data frame Data.5. Further, the First Principal Component is extracted from the result and the respective median is computed. This median value is a threshold for classifying the data points into two groups based on their position relative to the median along the first principal component axis. The data points are further classified as 1 for points above the median, 0 for points below the median. The actual class labels are extracted from the original data frame and are converted to character type for analysis. A contingency table is created with predicted and actual classes to execute the Fisher's exact test to check the association between the predicted and actual class labels.

The Confusion matrix shows the True negatives is 0,False Positives is 16, False Negatives is 0,True positives is 18. The Accuracy of classification of sample is 52.94 percent for this model.kappa value is 0 which represents that there is no agreement beyond the chance between the predicted and actual class.Specificity marks 52.94 percent,representing the negative samples of the model.McNemar's Test P-value is 0.0001768 which shows that the significant difference in error between the predicted and actual classes.Youden's Index is 0 as its cannot be calculated as sensitivity is not available.

The above Fisher's Exact test has a p-value 0.8196, which shows insufficiency to reject the null hypothesis.
This suggests a likelihood of no association between the predicted class on the first principal component and the actual class. The 95 percent confidence interval for the odds ratio ranges from 0.3010991 to 2.1825224. And the estimated odds ratio calculated as 0.8137785 depicts that the odds of the correct class based on the first component are slightly lower than the odds of the incorrect class. The difference is not significant.

To conclude, on the above Fisher's Exact test, there is insufficient evidence to conclude the association. Alternative methods or larger datasets are required for further validation. Hence, it concludes that there is no association between the predicted class based on the first principal component and the actual class from the dataset.






